{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index\n",
    "%pip install -U llama-parse\n",
    "%pip install llama-index-vector-stores-mongodb\n",
    "%pip install llama-index-llms-openai\n",
    "%pip install llama-index-llms-cohere\n",
    "%pip install llama-index-embeddings-cohere\n",
    "%pip install llama-index-postprocessor-cohere-rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"\"\n",
    "os.environ[\"MONGO_URI\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-parse is async-first, running the sync code in a notebook requires the use of nest_asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import pymongo\n",
    "\n",
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the document using `LlamaParse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When checking in January 2025, we can only pass one language when using the Python SDK.\n",
    "# You can specify multiple languages via the web UI.\n",
    "# Note that we are using Premium mode here.\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    premium_mode=True,\n",
    "    language=\"ch_tra\"\n",
    ")\n",
    "\n",
    "# Use SimpleDirectoryReader to parse our file\n",
    "file_extractor = {\".docx\": parser}\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=['path to your directory or file'],\n",
    "    file_extractor=file_extractor).load_data()\n",
    "print(documents)\n",
    "\n",
    "# documents = LlamaParse(result_type=\"text\").load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.llms.cohere import Cohere\n",
    "\n",
    "embed_model = CohereEmbedding(\n",
    "    api_key=os.environ[\"COHERE_API_KEY\"],\n",
    "    model_name=\"embed-multilingual-v3.0\",\n",
    "    input_type=\"search_document\",\n",
    ")\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = Cohere(model=\"command-r-plus\", api_key=os.environ[\"COHERE_API_KEY\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `MongoDBAtlasVectorSearch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_uri = os.environ[\"MONGO_URI\"]\n",
    "\n",
    "mongodb_client = pymongo.MongoClient(mongo_uri)\n",
    "atlas_vector_store = MongoDBAtlasVectorSearch(\n",
    "    mongodb_client,\n",
    "    db_name = \"llamaindex_db\",\n",
    "    collection_name = \"llamaparse\",\n",
    "    vector_index_name = \"vector_index\",\n",
    "\n",
    ")\n",
    "vector_store_context = StorageContext.from_defaults(vector_store=atlas_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Index and Query Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_index = VectorStoreIndex.from_documents(\n",
    "   documents,\n",
    "   storage_context=vector_store_context,\n",
    "   show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the collection for which to create the index\n",
    "collection = mongodb_client[\"llamaindex_db\"][\"llamaparse\"]\n",
    "# Create your index model, then create the search index\n",
    "from pymongo.operations import SearchIndexModel\n",
    "\n",
    "search_index_model = SearchIndexModel(\n",
    "  definition={\n",
    "    \"fields\": [\n",
    "      {\n",
    "        \"type\": \"vector\",\n",
    "        \"path\": \"embedding\",\n",
    "        \"numDimensions\": 1024,\n",
    "        \"similarity\": \"cosine\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  name=\"vector_index\",\n",
    "  type=\"vectorSearch\",\n",
    ")\n",
    "collection.create_search_index(model=search_index_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store_index.as_retriever(similarity_top_k=3)\n",
    "nodes = retriever.retrieve(\"What is the document about\")\n",
    "for node in nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Atlas Vector Search as a retriever\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "import pprint\n",
    "\n",
    "vector_store_retriever = VectorIndexRetriever(index=vector_store_index, similarity_top_k=5)\n",
    "\n",
    "# Pass the retriever into the query engine\n",
    "embed_model = CohereEmbedding(\n",
    "    api_key=os.environ[\"COHERE_API_KEY\"],\n",
    "    model_name=\"embed-multilingual-v3.0\",\n",
    "    input_type=\"search_query\",\n",
    ")\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "query_engine = RetrieverQueryEngine(retriever=vector_store_retriever)\n",
    "\n",
    "# Prompt the LLM\n",
    "response = query_engine.query(\"What is the document about\")\n",
    "\n",
    "print(response)\n",
    "print(\"\\nSource documents: \")\n",
    "pprint.pprint(response.source_nodes)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
